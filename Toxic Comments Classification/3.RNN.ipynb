{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Tools\\Miniconda3\\envs\\py_3_6_5_keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "d:\\Tools\\Miniconda3\\envs\\py_3_6_5_keras\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 5000 rows and 39 columns\n"
     ]
    }
   ],
   "source": [
    "#This dataset contains new features, ascii text converted to unicode and standardized\n",
    "nrows= 5000\n",
    "train = pd.read_csv('Data/train_processed_stage1.csv', nrows=nrows)\n",
    "print('Train dataset contains {} rows and {} columns'.format(*train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_clean_text_df contains 5000 rows and 2 columns\n",
      "X_meta_feats_df contains 5000 rows and 30 columns\n",
      "y contains 5000 rows and 6 columns\n"
     ]
    }
   ],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "meta_features_cols = ['total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks', 'num_question_marks', \\\n",
    " 'num_punctuation', 'num_symbols', 'num_words', 'num_unique_words', 'words_vs_unique', 'num_smilies', 'ant_slash_n', \\\n",
    " 'nb_fk', 'nb_sk', 'nb_dk', 'nb_you', 'nb_ng', 'nb_mother', 'start_with_columns', 'has_timestamp', 'has_date_long', \\\n",
    " 'has_date_short', 'has_http', 'has_mail', 'has_image', 'has_ip', 'has_emphasize_equal', 'has_emphasize_quotes', \\\n",
    " 'has_star', 'unknown_fasttext']\n",
    "\n",
    "X_clean_text_df = train[['id','clean_text']].fillna('something')\n",
    "X_meta_feats_df = train[meta_features_cols].fillna('something')\n",
    "y = train[class_names]\n",
    "print ('X_clean_text_df contains {} rows and {} columns'.format(*X_clean_text_df.shape))\n",
    "print ('X_meta_feats_df contains {} rows and {} columns'.format(*X_meta_feats_df.shape))\n",
    "print ('y contains {} rows and {} columns'.format(*y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>d aww ! he matches this background colour i am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>more i can not make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                         clean_text\n",
       "0  0000997932d777bf  explanation why the edits made under my userna...\n",
       "1  000103f0d9cfb60f  d aww ! he matches this background colour i am...\n",
       "2  000113f07ec002fd  hey man i am really not trying to edit war it ...\n",
       "3  0001b41b1c6bb37e  more i can not make any real suggestions on im...\n",
       "4  0001d958c54c6e35  you sir are my hero any chance you remember wh..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>...</th>\n",
       "      <th>has_date_long</th>\n",
       "      <th>has_date_short</th>\n",
       "      <th>has_http</th>\n",
       "      <th>has_mail</th>\n",
       "      <th>has_image</th>\n",
       "      <th>has_ip</th>\n",
       "      <th>has_emphasize_equal</th>\n",
       "      <th>has_emphasize_quotes</th>\n",
       "      <th>has_star</th>\n",
       "      <th>unknown_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.220195</td>\n",
       "      <td>-0.001654</td>\n",
       "      <td>0.137572</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>0.346388</td>\n",
       "      <td>-0.154587</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.244618</td>\n",
       "      <td>-0.130379</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>1.340625</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.477509</td>\n",
       "      <td>-0.094891</td>\n",
       "      <td>0.213346</td>\n",
       "      <td>0.013065</td>\n",
       "      <td>-0.282584</td>\n",
       "      <td>-0.232894</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.506634</td>\n",
       "      <td>-0.571261</td>\n",
       "      <td>1.131109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.272673</td>\n",
       "      <td>-0.136330</td>\n",
       "      <td>-0.371132</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>-0.282584</td>\n",
       "      <td>-0.311200</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.254695</td>\n",
       "      <td>-0.167119</td>\n",
       "      <td>0.573838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.385847</td>\n",
       "      <td>-0.063812</td>\n",
       "      <td>-0.365558</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>-0.282584</td>\n",
       "      <td>-0.154587</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>0.460811</td>\n",
       "      <td>0.622796</td>\n",
       "      <td>-1.009204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.553687</td>\n",
       "      <td>-0.157049</td>\n",
       "      <td>-0.234513</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>0.346388</td>\n",
       "      <td>-0.389507</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.546945</td>\n",
       "      <td>-0.644741</td>\n",
       "      <td>1.131109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_length  capitals  caps_vs_length  num_exclamation_marks  \\\n",
       "0     -0.220195 -0.001654        0.137572              -0.025546   \n",
       "1     -0.477509 -0.094891        0.213346               0.013065   \n",
       "2     -0.272673 -0.136330       -0.371132              -0.025546   \n",
       "3      0.385847 -0.063812       -0.365558              -0.025546   \n",
       "4     -0.553687 -0.157049       -0.234513              -0.025546   \n",
       "\n",
       "   num_question_marks  num_punctuation  num_symbols  num_words  \\\n",
       "0            0.346388        -0.154587    -0.107151  -0.244618   \n",
       "1           -0.282584        -0.232894    -0.107151  -0.506634   \n",
       "2           -0.282584        -0.311200    -0.107151  -0.254695   \n",
       "3           -0.282584        -0.154587    -0.107151   0.460811   \n",
       "4            0.346388        -0.389507    -0.107151  -0.546945   \n",
       "\n",
       "   num_unique_words  words_vs_unique        ...         has_date_long  \\\n",
       "0         -0.130379         0.768235        ...             -0.081772   \n",
       "1         -0.571261         1.131109        ...             -0.081772   \n",
       "2         -0.167119         0.573838        ...             -0.081772   \n",
       "3          0.622796        -1.009204        ...             -0.081772   \n",
       "4         -0.644741         1.131109        ...             -0.081772   \n",
       "\n",
       "   has_date_short  has_http  has_mail  has_image    has_ip  \\\n",
       "0       -0.112575 -0.129103 -0.028225  -0.008183  1.340625   \n",
       "1       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "2       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "3       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "4       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "\n",
       "   has_emphasize_equal  has_emphasize_quotes  has_star  unknown_fasttext  \n",
       "0            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "1            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "2            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "3            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "4            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta_feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0\n",
       "5      0             0        0       0       0              0\n",
       "6      1             1        1       0       1              0\n",
       "7      0             0        0       0       0              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert clean_text to vectors/tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This allows to vectorize a text corpus, by turning each text into a sequence of integers \n",
    "src_vocab_size = 10000\n",
    "# src_vocab_size = 283759\n",
    "def create_tokenizer():\n",
    "    #consider only the top 10000 words in the dataset\n",
    "    tokenizer = text.Tokenizer(num_words=src_vocab_size)\n",
    "    tokenizer.fit_on_texts(list(X_clean_text_df['clean_text']))\n",
    "    return tokenizer\n",
    "\n",
    "def find_max_len(percentile=0.9):\n",
    "    #since each of the reviews can be of varying length. we will convert them to be of same length by padding.\n",
    "    #lets find out what should be the length of each reviews\n",
    "    #find the length\n",
    "    X_clean_text_df['text_length'] = X_clean_text_df['clean_text'].apply(len)\n",
    "\n",
    "    # use the 90th percentile to pad sequences\n",
    "    maxlen = int(X_clean_text_df.text_length.quantile(percentile)) # 90th percentile\n",
    "    return maxlen\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length):\n",
    "    #map words to integers as needed for modelling\n",
    "    X_train_sequence = tokenizer.texts_to_sequences(X_clean_text_df['clean_text'])\n",
    "\n",
    "    # pad sequences with 0 values\n",
    "    #it will take each review and make them to be of 836 words.\n",
    "    #if the review is less than 836 words it will pad zeroes at the start and append the reviews to the end.\n",
    "    #if the review is more than 836 words, the words beyond 600 will be truncated\n",
    "    X_train_padded_seq = sequence.pad_sequences(X_train_sequence, maxlen=length)\n",
    "    return X_train_padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens created from the training dataset is 22966\n",
      "Sequences will be of length - 837\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer()\n",
    "print('Number of unique tokens created from the training dataset is {}'.format(len(tokenizer.word_index)))\n",
    "\n",
    "maxlen = find_max_len()\n",
    "print('Sequences will be of length - {}'.format(maxlen))\n",
    "\n",
    "X_train_padded_seq = encode_sequences(tokenizer, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   41, 3220,   88],\n",
       "       [   0,    0,    0, ...,   37,  934,  179]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_seq[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Build an embedding matrix that can be loaded into an Embedding layer. It must be a matrix of shape (src_vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and -0.1081 0.0191 0.0354 0.0127 0.0664 -0.0126 -0.1882 0.0631 -0.2306 0.0095 0.0917 0.1513 0.0558 -0.0643 -0.0288 -0.0447 0.1603 0.0613 0.0349 -0.0578 0.0003 -0.1399 0.0163 -0.0419 -0.0487 0.0057 0.0017 -0.0268 -0.0170 0.1045 0.0815 -0.0060 0.0635 -0.1710 0.0276 -0.0230 0.0824 0.0304 -0.1595 0.0851 -0.0556 0.0329 -0.0921 0.0196 -0.0404 0.0361 -0.0843 0.1014 -0.0393 -0.1805 -0.0073 0.2247 -0.0621 -0.0575 -0.0316 0.0198 0.0602 0.1312 -0.1278 0.0177 -0.0600 -0.2904 -0.0465 -0.1078 -0.0701 -0.0497 -0.1102 0.0156 -0.0595 0.0762 0.2638 0.0353 0.0745 0.0361 -0.0561 -0.1179 -0.4751 0.0458 -0.0672 -0.0022 0.9264 0.0101 -0.1085 0.0669 -0.0417 -0.0428 0.0391 -0.0581 0.0107 -0.0873 -0.0016 0.5711 0.0345 0.0930 -0.0647 -0.0820 0.0587 -0.1307 0.0480 0.0226 0.0115 0.0296 -0.1675 0.0399 0.2511 0.0423 0.1299 -0.0336 -0.0977 0.0146 0.3527 0.0122 0.1031 -0.0657 -0.3704 0.0445 0.1242 -0.0031 0.0255 -0.0109 0.1575 0.0288 0.2549 -0.0280 0.0462 -0.0282 -0.0154 0.0405 0.3191 0.0113 -0.0136 0.0182 -0.0385 -0.0499 -0.0810 -0.0134 -0.1413 0.0566 -0.0423 -0.0609 0.1063 0.0386 -0.0375 0.2714 -0.0665 -0.0219 0.0579 0.0550 0.0772 0.2024 -0.0365 -0.0764 0.0318 0.0004 -0.0424 -0.0579 0.0428 0.0211 -0.0061 0.0243 0.5702 0.0798 -0.0502 -0.0195 0.0242 -0.0339 -0.0441 -0.1095 0.0426 -0.0234 -0.0057 -0.0209 -0.0903 -0.0654 0.1559 0.0051 0.0037 -0.0008 -0.0050 0.0508 0.0105 0.0251 0.0270 0.0039 0.0621 0.0519 -0.0156 -0.1285 -0.0564 -0.0263 0.0401 0.0372 -0.1040 -0.0674 0.0866 -0.0033 -0.0076 0.0064 -0.0114 -0.0239 0.0135 0.0801 0.0171 0.0425 -0.0499 0.0615 0.0355 -0.0833 0.0378 0.0288 -0.0392 0.5679 -0.0120 0.0699 -0.0893 -0.0105 0.0219 -0.0136 0.1063 -0.0014 -0.0062 -0.0617 0.0357 0.0030 -0.0414 -0.0634 0.0701 0.0501 0.0241 0.0379 -0.0234 -0.0116 0.0391 -0.0169 -0.0019 0.0731 0.0597 0.0191 -0.0262 -0.0256 0.0631 -0.0261 0.0185 0.0100 0.1126 -0.0487 0.1545 0.0438 0.0147 -0.0278 -0.0493 -0.0866 -0.0281 -0.0955 0.0879 0.0194 -0.0323 -0.0126 -0.0810 0.0088 0.0002 0.0059 0.0858 0.0024 0.0401 -0.0205 0.0304 0.0678 0.0566 0.0097 0.0039 -0.0968 0.0079 -0.3838 -0.0642 -0.0417 -0.0666 0.0337 0.0053 -0.0263 -0.0310 0.2002 -0.0096 0.0796 0.0743 0.0709 -0.0077 -0.0169 -0.0083 -0.0322 0.0608 0.0227 -0.0306 0.3945 0.0200 -0.0613 -0.0611 0.1104 0.0475 -0.0599 \n",
      "\n",
      "('and', array([-1.081e-01,  1.910e-02,  3.540e-02,  1.270e-02,  6.640e-02,\n",
      "       -1.260e-02, -1.882e-01,  6.310e-02, -2.306e-01,  9.500e-03,\n",
      "        9.170e-02,  1.513e-01,  5.580e-02, -6.430e-02, -2.880e-02,\n",
      "       -4.470e-02,  1.603e-01,  6.130e-02,  3.490e-02, -5.780e-02,\n",
      "        3.000e-04, -1.399e-01,  1.630e-02, -4.190e-02, -4.870e-02,\n",
      "        5.700e-03,  1.700e-03, -2.680e-02, -1.700e-02,  1.045e-01,\n",
      "        8.150e-02, -6.000e-03,  6.350e-02, -1.710e-01,  2.760e-02,\n",
      "       -2.300e-02,  8.240e-02,  3.040e-02, -1.595e-01,  8.510e-02,\n",
      "       -5.560e-02,  3.290e-02, -9.210e-02,  1.960e-02, -4.040e-02,\n",
      "        3.610e-02, -8.430e-02,  1.014e-01, -3.930e-02, -1.805e-01,\n",
      "       -7.300e-03,  2.247e-01, -6.210e-02, -5.750e-02, -3.160e-02,\n",
      "        1.980e-02,  6.020e-02,  1.312e-01, -1.278e-01,  1.770e-02,\n",
      "       -6.000e-02, -2.904e-01, -4.650e-02, -1.078e-01, -7.010e-02,\n",
      "       -4.970e-02, -1.102e-01,  1.560e-02, -5.950e-02,  7.620e-02,\n",
      "        2.638e-01,  3.530e-02,  7.450e-02,  3.610e-02, -5.610e-02,\n",
      "       -1.179e-01, -4.751e-01,  4.580e-02, -6.720e-02, -2.200e-03,\n",
      "        9.264e-01,  1.010e-02, -1.085e-01,  6.690e-02, -4.170e-02,\n",
      "       -4.280e-02,  3.910e-02, -5.810e-02,  1.070e-02, -8.730e-02,\n",
      "       -1.600e-03,  5.711e-01,  3.450e-02,  9.300e-02, -6.470e-02,\n",
      "       -8.200e-02,  5.870e-02, -1.307e-01,  4.800e-02,  2.260e-02,\n",
      "        1.150e-02,  2.960e-02, -1.675e-01,  3.990e-02,  2.511e-01,\n",
      "        4.230e-02,  1.299e-01, -3.360e-02, -9.770e-02,  1.460e-02,\n",
      "        3.527e-01,  1.220e-02,  1.031e-01, -6.570e-02, -3.704e-01,\n",
      "        4.450e-02,  1.242e-01, -3.100e-03,  2.550e-02, -1.090e-02,\n",
      "        1.575e-01,  2.880e-02,  2.549e-01, -2.800e-02,  4.620e-02,\n",
      "       -2.820e-02, -1.540e-02,  4.050e-02,  3.191e-01,  1.130e-02,\n",
      "       -1.360e-02,  1.820e-02, -3.850e-02, -4.990e-02, -8.100e-02,\n",
      "       -1.340e-02, -1.413e-01,  5.660e-02, -4.230e-02, -6.090e-02,\n",
      "        1.063e-01,  3.860e-02, -3.750e-02,  2.714e-01, -6.650e-02,\n",
      "       -2.190e-02,  5.790e-02,  5.500e-02,  7.720e-02,  2.024e-01,\n",
      "       -3.650e-02, -7.640e-02,  3.180e-02,  4.000e-04, -4.240e-02,\n",
      "       -5.790e-02,  4.280e-02,  2.110e-02, -6.100e-03,  2.430e-02,\n",
      "        5.702e-01,  7.980e-02, -5.020e-02, -1.950e-02,  2.420e-02,\n",
      "       -3.390e-02, -4.410e-02, -1.095e-01,  4.260e-02, -2.340e-02,\n",
      "       -5.700e-03, -2.090e-02, -9.030e-02, -6.540e-02,  1.559e-01,\n",
      "        5.100e-03,  3.700e-03, -8.000e-04, -5.000e-03,  5.080e-02,\n",
      "        1.050e-02,  2.510e-02,  2.700e-02,  3.900e-03,  6.210e-02,\n",
      "        5.190e-02, -1.560e-02, -1.285e-01, -5.640e-02, -2.630e-02,\n",
      "        4.010e-02,  3.720e-02, -1.040e-01, -6.740e-02,  8.660e-02,\n",
      "       -3.300e-03, -7.600e-03,  6.400e-03, -1.140e-02, -2.390e-02,\n",
      "        1.350e-02,  8.010e-02,  1.710e-02,  4.250e-02, -4.990e-02,\n",
      "        6.150e-02,  3.550e-02, -8.330e-02,  3.780e-02,  2.880e-02,\n",
      "       -3.920e-02,  5.679e-01, -1.200e-02,  6.990e-02, -8.930e-02,\n",
      "       -1.050e-02,  2.190e-02, -1.360e-02,  1.063e-01, -1.400e-03,\n",
      "       -6.200e-03, -6.170e-02,  3.570e-02,  3.000e-03, -4.140e-02,\n",
      "       -6.340e-02,  7.010e-02,  5.010e-02,  2.410e-02,  3.790e-02,\n",
      "       -2.340e-02, -1.160e-02,  3.910e-02, -1.690e-02, -1.900e-03,\n",
      "        7.310e-02,  5.970e-02,  1.910e-02, -2.620e-02, -2.560e-02,\n",
      "        6.310e-02, -2.610e-02,  1.850e-02,  1.000e-02,  1.126e-01,\n",
      "       -4.870e-02,  1.545e-01,  4.380e-02,  1.470e-02, -2.780e-02,\n",
      "       -4.930e-02, -8.660e-02, -2.810e-02, -9.550e-02,  8.790e-02,\n",
      "        1.940e-02, -3.230e-02, -1.260e-02, -8.100e-02,  8.800e-03,\n",
      "        2.000e-04,  5.900e-03,  8.580e-02,  2.400e-03,  4.010e-02,\n",
      "       -2.050e-02,  3.040e-02,  6.780e-02,  5.660e-02,  9.700e-03,\n",
      "        3.900e-03, -9.680e-02,  7.900e-03, -3.838e-01, -6.420e-02,\n",
      "       -4.170e-02, -6.660e-02,  3.370e-02,  5.300e-03, -2.630e-02,\n",
      "       -3.100e-02,  2.002e-01, -9.600e-03,  7.960e-02,  7.430e-02,\n",
      "        7.090e-02, -7.700e-03, -1.690e-02, -8.300e-03, -3.220e-02,\n",
      "        6.080e-02,  2.270e-02, -3.060e-02,  3.945e-01,  2.000e-02,\n",
      "       -6.130e-02, -6.110e-02,  1.104e-01,  4.750e-02, -5.990e-02],\n",
      "      dtype=float32))\n",
      "to -0.0175 -0.2189 0.0353 0.0345 0.0913 0.0269 -0.1670 -0.2759 0.4283 0.0349 0.0132 0.1078 -0.0558 -0.0215 -0.0389 0.0379 -0.1180 0.0164 0.0631 0.0459 0.0331 -0.0176 0.0010 0.0176 0.0351 -0.0545 0.0430 -0.0300 0.0358 0.2588 0.0576 -0.0854 0.0632 0.0255 0.0008 0.0152 -0.0843 -0.0249 0.0651 -0.0961 -0.0107 0.1649 0.0003 -0.2875 -0.0388 -0.0212 -0.0973 0.0301 -0.0474 0.9173 -0.0024 0.4422 0.1367 -0.0177 -0.0591 -0.0069 0.0692 -0.3956 -0.0486 0.0251 0.1376 0.1816 -0.0164 -0.2103 0.0156 -0.0449 -0.1004 0.0399 -0.0847 -0.0565 -0.2697 0.0051 -0.0441 0.1862 -0.0143 -0.1114 -0.7036 0.0162 0.1955 -0.0225 0.6761 0.0295 -0.0039 -0.0450 -0.0383 0.0258 0.0464 -0.0243 0.0243 0.0037 0.0031 0.6531 0.0330 0.0092 0.0026 0.0351 -0.3749 -0.0402 -0.2596 0.0326 0.0055 -0.0039 0.1751 0.0420 0.1438 -0.0109 0.0236 -0.0084 -0.0188 0.1287 0.0624 0.1152 0.0244 0.0207 -0.3837 0.2028 0.1272 0.0179 0.1020 0.0433 -0.0650 0.0281 0.1151 -0.0074 0.0174 -0.0252 0.0533 0.0659 0.2928 -0.0211 -0.0252 -0.0004 -0.0462 -0.0615 0.3430 -0.0705 -0.0938 0.0509 0.0076 -0.2799 0.1191 0.0606 -0.0089 0.1064 -0.0829 -0.0044 -0.1018 -0.0100 0.0765 0.1046 0.0296 -0.0144 -0.0241 0.0203 -0.0177 -0.0347 0.0244 0.0044 0.0060 0.0126 0.4667 0.1586 -0.0376 -0.0566 -0.0387 0.0108 -0.0083 -0.0498 0.0121 -0.0458 0.0430 0.0027 -0.0922 0.0159 0.0770 -0.0638 0.0435 -0.0152 0.0018 -0.0024 -0.2509 -0.0197 0.0097 0.0214 0.0603 -0.0105 -0.0940 -0.4068 -0.2884 0.0445 0.0020 0.0126 -0.0744 -0.0678 0.0483 -0.0919 0.0431 -0.0401 0.0313 -0.0202 0.0297 0.0010 0.0224 0.0575 0.0349 -0.0103 0.0015 0.0122 0.0415 -0.2559 0.0132 0.5005 -0.0074 0.0416 -0.0839 0.0086 -0.0155 -0.0089 -0.0076 0.0295 -0.0277 -0.0942 0.0236 0.0153 0.0798 -0.0475 -0.0431 0.0369 0.0396 0.0146 0.0790 0.0716 0.0334 -0.0462 0.0121 0.0620 0.0960 -0.0337 -0.0617 0.0416 -0.0236 0.0054 -0.1031 -0.0331 -0.3805 -0.0750 0.4056 0.0209 0.0106 -0.0344 0.0162 -0.1616 0.0356 -0.0108 0.1220 0.0484 0.0353 0.0462 -0.0053 0.0145 0.0615 0.0071 -0.0241 -0.0327 0.0071 -0.1290 0.0202 0.0345 0.0275 -0.0193 -0.0033 -0.4104 0.0505 -0.1699 -0.0398 -0.0099 -0.3296 -0.0402 0.1344 0.0010 0.0336 0.1412 0.0023 -0.0556 0.0348 -0.0625 -0.0228 -0.0015 0.0732 0.0362 0.0642 0.0136 0.0367 0.1735 0.0217 0.0591 0.0166 -0.2846 0.0509 0.0229 \n",
      "\n",
      "('to', array([-1.750e-02, -2.189e-01,  3.530e-02,  3.450e-02,  9.130e-02,\n",
      "        2.690e-02, -1.670e-01, -2.759e-01,  4.283e-01,  3.490e-02,\n",
      "        1.320e-02,  1.078e-01, -5.580e-02, -2.150e-02, -3.890e-02,\n",
      "        3.790e-02, -1.180e-01,  1.640e-02,  6.310e-02,  4.590e-02,\n",
      "        3.310e-02, -1.760e-02,  1.000e-03,  1.760e-02,  3.510e-02,\n",
      "       -5.450e-02,  4.300e-02, -3.000e-02,  3.580e-02,  2.588e-01,\n",
      "        5.760e-02, -8.540e-02,  6.320e-02,  2.550e-02,  8.000e-04,\n",
      "        1.520e-02, -8.430e-02, -2.490e-02,  6.510e-02, -9.610e-02,\n",
      "       -1.070e-02,  1.649e-01,  3.000e-04, -2.875e-01, -3.880e-02,\n",
      "       -2.120e-02, -9.730e-02,  3.010e-02, -4.740e-02,  9.173e-01,\n",
      "       -2.400e-03,  4.422e-01,  1.367e-01, -1.770e-02, -5.910e-02,\n",
      "       -6.900e-03,  6.920e-02, -3.956e-01, -4.860e-02,  2.510e-02,\n",
      "        1.376e-01,  1.816e-01, -1.640e-02, -2.103e-01,  1.560e-02,\n",
      "       -4.490e-02, -1.004e-01,  3.990e-02, -8.470e-02, -5.650e-02,\n",
      "       -2.697e-01,  5.100e-03, -4.410e-02,  1.862e-01, -1.430e-02,\n",
      "       -1.114e-01, -7.036e-01,  1.620e-02,  1.955e-01, -2.250e-02,\n",
      "        6.761e-01,  2.950e-02, -3.900e-03, -4.500e-02, -3.830e-02,\n",
      "        2.580e-02,  4.640e-02, -2.430e-02,  2.430e-02,  3.700e-03,\n",
      "        3.100e-03,  6.531e-01,  3.300e-02,  9.200e-03,  2.600e-03,\n",
      "        3.510e-02, -3.749e-01, -4.020e-02, -2.596e-01,  3.260e-02,\n",
      "        5.500e-03, -3.900e-03,  1.751e-01,  4.200e-02,  1.438e-01,\n",
      "       -1.090e-02,  2.360e-02, -8.400e-03, -1.880e-02,  1.287e-01,\n",
      "        6.240e-02,  1.152e-01,  2.440e-02,  2.070e-02, -3.837e-01,\n",
      "        2.028e-01,  1.272e-01,  1.790e-02,  1.020e-01,  4.330e-02,\n",
      "       -6.500e-02,  2.810e-02,  1.151e-01, -7.400e-03,  1.740e-02,\n",
      "       -2.520e-02,  5.330e-02,  6.590e-02,  2.928e-01, -2.110e-02,\n",
      "       -2.520e-02, -4.000e-04, -4.620e-02, -6.150e-02,  3.430e-01,\n",
      "       -7.050e-02, -9.380e-02,  5.090e-02,  7.600e-03, -2.799e-01,\n",
      "        1.191e-01,  6.060e-02, -8.900e-03,  1.064e-01, -8.290e-02,\n",
      "       -4.400e-03, -1.018e-01, -1.000e-02,  7.650e-02,  1.046e-01,\n",
      "        2.960e-02, -1.440e-02, -2.410e-02,  2.030e-02, -1.770e-02,\n",
      "       -3.470e-02,  2.440e-02,  4.400e-03,  6.000e-03,  1.260e-02,\n",
      "        4.667e-01,  1.586e-01, -3.760e-02, -5.660e-02, -3.870e-02,\n",
      "        1.080e-02, -8.300e-03, -4.980e-02,  1.210e-02, -4.580e-02,\n",
      "        4.300e-02,  2.700e-03, -9.220e-02,  1.590e-02,  7.700e-02,\n",
      "       -6.380e-02,  4.350e-02, -1.520e-02,  1.800e-03, -2.400e-03,\n",
      "       -2.509e-01, -1.970e-02,  9.700e-03,  2.140e-02,  6.030e-02,\n",
      "       -1.050e-02, -9.400e-02, -4.068e-01, -2.884e-01,  4.450e-02,\n",
      "        2.000e-03,  1.260e-02, -7.440e-02, -6.780e-02,  4.830e-02,\n",
      "       -9.190e-02,  4.310e-02, -4.010e-02,  3.130e-02, -2.020e-02,\n",
      "        2.970e-02,  1.000e-03,  2.240e-02,  5.750e-02,  3.490e-02,\n",
      "       -1.030e-02,  1.500e-03,  1.220e-02,  4.150e-02, -2.559e-01,\n",
      "        1.320e-02,  5.005e-01, -7.400e-03,  4.160e-02, -8.390e-02,\n",
      "        8.600e-03, -1.550e-02, -8.900e-03, -7.600e-03,  2.950e-02,\n",
      "       -2.770e-02, -9.420e-02,  2.360e-02,  1.530e-02,  7.980e-02,\n",
      "       -4.750e-02, -4.310e-02,  3.690e-02,  3.960e-02,  1.460e-02,\n",
      "        7.900e-02,  7.160e-02,  3.340e-02, -4.620e-02,  1.210e-02,\n",
      "        6.200e-02,  9.600e-02, -3.370e-02, -6.170e-02,  4.160e-02,\n",
      "       -2.360e-02,  5.400e-03, -1.031e-01, -3.310e-02, -3.805e-01,\n",
      "       -7.500e-02,  4.056e-01,  2.090e-02,  1.060e-02, -3.440e-02,\n",
      "        1.620e-02, -1.616e-01,  3.560e-02, -1.080e-02,  1.220e-01,\n",
      "        4.840e-02,  3.530e-02,  4.620e-02, -5.300e-03,  1.450e-02,\n",
      "        6.150e-02,  7.100e-03, -2.410e-02, -3.270e-02,  7.100e-03,\n",
      "       -1.290e-01,  2.020e-02,  3.450e-02,  2.750e-02, -1.930e-02,\n",
      "       -3.300e-03, -4.104e-01,  5.050e-02, -1.699e-01, -3.980e-02,\n",
      "       -9.900e-03, -3.296e-01, -4.020e-02,  1.344e-01,  1.000e-03,\n",
      "        3.360e-02,  1.412e-01,  2.300e-03, -5.560e-02,  3.480e-02,\n",
      "       -6.250e-02, -2.280e-02, -1.500e-03,  7.320e-02,  3.620e-02,\n",
      "        6.420e-02,  1.360e-02,  3.670e-02,  1.735e-01,  2.170e-02,\n",
      "        5.910e-02,  1.660e-02, -2.846e-01,  5.090e-02,  2.290e-02],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "#understand what the file contains\n",
    "EMBEDDING_FILE_FASTTEXT=\"Data/embeddings/crawl-300d-2M.vec\"\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "cnt = 1\n",
    "for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'):\n",
    "    if cnt > 4:\n",
    "        print(o)\n",
    "        print(get_coefs(*o.rstrip().rsplit(' ')))\n",
    "    cnt += 1\n",
    "    if cnt > 6: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def create_embeddings(source_file):\n",
    "    embeddings = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(source_file,encoding='utf-8'))\n",
    "    return embeddings\n",
    "\n",
    "def save_embeddings_to_file(source_file, target_file_name):\n",
    "    with open(target_file_name, 'wb') as handle:\n",
    "        pickle.dump(source_file, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_pickle_file():\n",
    "    if os.path.isfile(EMBEDDING_DICT_FASTTEXT):\n",
    "        #load file\n",
    "        with open(EMBEDDING_DICT_FASTTEXT, 'rb') as handle:\n",
    "            embeddings_index_ft = pickle.load(handle)\n",
    "    else:\n",
    "        embeddings_index_ft = create_embeddings(EMBEDDING_FILE_FASTTEXT)\n",
    "        save_embeddings_to_file(embeddings_index_ft, EMBEDDING_DICT_FASTTEXT)\n",
    "        \n",
    "    if os.path.isfile(EMBEDDING_DICT_TWITTER):\n",
    "        #load file\n",
    "        with open(EMBEDDING_DICT_TWITTER, 'rb') as handle:\n",
    "            embeddings_index_tw = pickle.load(handle)\n",
    "    else:\n",
    "        embeddings_index_tw = create_embeddings(EMBEDDING_FILE_TWITTER)\n",
    "        save_embeddings_to_file(embeddings_index_tw, EMBEDDING_DICT_TWITTER)\n",
    "    return embeddings_index_ft, embeddings_index_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 word vectors from fasttext embeddings.\n",
      "Loaded 1193514 word vectors from twitter embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Load the FastText Web Crawl vectors\n",
    "# The emebedding file contains words and its corresponding vectors. \n",
    "# For instance the word and will be stored in embedding file as\n",
    "# (and -0.1081 0.0191 0.0354 0.0127 0.0664 -0.0126 -0.1882 0.0631 -0.2306 0.0095 0.0917 0.1513)\n",
    "# we will create a dictionary where the word will be the key and the vectors will be an array\n",
    "EMBEDDING_FILE_FASTTEXT=\"Data/embeddings/crawl-300d-2M.vec\"\n",
    "EMBEDDING_FILE_TWITTER=\"Data/embeddings/glove.twitter.27B.200d.txt\"\n",
    "\n",
    "EMBEDDING_DICT_FASTTEXT=\"Data/embeddings/embeddings_index_ft.pickle\"\n",
    "EMBEDDING_DICT_TWITTER=\"Data/embeddings/embeddings_index_tw.pickle\"\n",
    "\n",
    "# load the fasttext and twitter word embeddings file into memory as a dictionary of word to embedding array.\n",
    "#This process is slow so it is better to save the files\n",
    "embeddings_index_ft, embeddings_index_tw = load_embeddings_from_pickle_file()\n",
    "print('Loaded {} word vectors from fasttext embeddings.'.format(len(embeddings_index_ft)))\n",
    "print('Loaded {} word vectors from twitter embeddings.'.format(len(embeddings_index_tw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 03:24:57 PM\n",
      "End time is 03:34:03 PM\n"
     ]
    }
   ],
   "source": [
    "#load spelling model. This will used to correct words that are not in the fasttext embeddings. \n",
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print('Start time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))\n",
    "\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print('End time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(spell_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is  based on: Spellchecker using Word2vec by CPMP\n",
    "# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "\n",
    "#get the list of words\n",
    "words = spell_model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "#word and an index as its value\n",
    "WORDS = w_rank\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "word_index = tokenizer.word_index\n",
    "i = 1\n",
    "for k,v in word_index.items():\n",
    "    print(k,v)\n",
    "    i +=1\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22966, 10000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index), src_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 04:23:00 PM\n",
      "End time is 04:23:58 PM\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print('Start time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))\n",
    "\n",
    "\n",
    "#this contain the word and its index, from the training dataset such as\n",
    "# the 1\n",
    "# to 2\n",
    "# i 3\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#len(word_index) is 22966 and src_vocab_size is 10000\n",
    "nb_words = min(src_vocab_size, len(word_index)) + 1\n",
    "\n",
    "\n",
    "# we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating \n",
    "# all unique words in the word_index and locating the embedding weight vector from the loaded fasttext embedding. \n",
    "# The result is a matrix of weights only for words we will see during training.\n",
    "# if the word is not found in the fasttext embedding we will try to correct the spelling and check\n",
    "embedding_matrix = np.zeros((nb_words, 501))\n",
    "\n",
    "#get embeddings for the word 'something' from twitter \n",
    "something_tw = embeddings_index_tw.get(\"something\")\n",
    "\n",
    "#get embeddings for the word 'something' from fasttext \n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((501,))\n",
    "something[:300,] = something_ft\n",
    "something[300:500,] = something_tw\n",
    "something[500,] = 0\n",
    "\n",
    "def all_caps(word):\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix, i, word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None: \n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_matrix[i,:300] = embedding_vector_ft\n",
    "        embedding_matrix[i,500] = last_value\n",
    "        embedding_vector_tw = embeddings_index_tw.get(word)\n",
    "        if embedding_vector_tw is not None:\n",
    "            embedding_matrix[i,300:500] = embedding_vector_tw\n",
    "\n",
    "            \n",
    "# Fasttext vector is used by itself if there is no glove vector but not the other way around.\n",
    "ctr = len(word_index)\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i >= src_vocab_size: continue\n",
    "    \n",
    "    if (ctr % 25000==0): print('{} more to go'.format(ctr))\n",
    "    ctr -= 1\n",
    "    \n",
    "    #is the word available in fasttext, create embeddings from fasttext vectors\n",
    "    #if not try to correct the word\n",
    "    if embeddings_index_ft.get(word) is not None:\n",
    "        embed_word(embedding_matrix,i,word)\n",
    "    else:\n",
    "        # change to > 20 for better score.\n",
    "        if len(word) > 20:\n",
    "            #use the something vectors\n",
    "            embedding_matrix[i] = something\n",
    "        else:\n",
    "            word2 = correction(word)\n",
    "            if embeddings_index_ft.get(word2) is not None:\n",
    "                embed_word(embedding_matrix,i,word2)\n",
    "            else:\n",
    "                word2 = correction(singlify(word))\n",
    "                if embeddings_index_ft.get(word2) is not None:\n",
    "                    embed_word(embedding_matrix,i,word2)\n",
    "                else:\n",
    "                    embedding_matrix[i] = something   \n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print('End time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
    "#                 model.save_weights(\"best_weights.h5\")\n",
    "                model.save_weights(bst_model_path)\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 3:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(features, max_features, clipvalue=1.,num_filters=40,dropout=0.5,embedding_dims=501):\n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "    # Layer 1: concatenated fasttext and glove twitter embeddings.\n",
    "    #There are multiple ways to represent a word as a vector: tfidf, bag of words, one-hot encoding etc. These are high\n",
    "    #dimensional sparse representation of the data. The other way to reporesent a word as a vector is us iing word embeddings-\n",
    "    #these are dense vectors low dimensional vectors in the form of 256 dim, 512 dim or 1024 dimensions. Word embeddings\n",
    "    #pack more information into fewer dimensions.\n",
    "    #There are 2 ways to obtain word embeddings:\n",
    "    #1. Learn word embeddings as part of the main task at hand such as classification.\n",
    "    #2. Load a pre trained word embedding. These embeddings are useful when there is less training data.\n",
    "    \n",
    "    #src_vocab_size represents what top n features we are using from the dataset\n",
    "    #embedding_dims represents the dimensions of the fasttext word embedding.\n",
    "    #embedding_matrix contains the embedding vectors for our training dataset\n",
    "    #by setting trainable=False, we are saying do not train this layer.\n",
    "    \n",
    "#     x = Embedding(src_vocab_size, embedding_dims, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = Embedding(max_features, embedding_dims, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    # Uncomment for best result\n",
    "    # Layer 2: SpatialDropout1D(0.5)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Uncomment for best result\n",
    "    # Layer 3: Bidirectional CuDNNLSTM\n",
    "    #a Bidirectional layer creates a second separate instance of the recurrent layer and uses one instance for processing\n",
    "    #the input sequences in chronological order and the other instance for processing the input sequences in reversed\n",
    "    #order. This looks at its input sequence both ways obtaining potentially richer reprsentations and capturing patterns\n",
    "    #that may have been missed by the chronological order version alone.\n",
    "    #create bidirectional layer with 40 such instances\n",
    "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
    "\n",
    "\n",
    "    # Layer 4: Bidirectional CuDNNGRU\n",
    "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n",
    "    \n",
    "    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n",
    "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
    "    \n",
    "    #if you want to detect the presence of something in your sequences, max pooling seems a good option.\n",
    "    #But if the contribution of the entire sequence seems important to your result, then average pooling sounds reasonable.\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = concatenate([avg_pool, x_h, max_pool,features_input])\n",
    "    \n",
    "    # Layer 6: output dense layer.\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp,features_input], outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 837)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 837, 501)     5010501     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 837, 501)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 837, 80)      173440      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) [(None, 837, 80), (N 29040       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 80)           0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 80)           0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 230)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 bidirectional_2[0][1]            \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1386        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 5,214,367\n",
      "Trainable params: 203,866\n",
      "Non-trainable params: 5,010,501\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, Embedding, SpatialDropout1D, concatenate, Dense\n",
    "# , , Dropout, Conv1D, , , \n",
    "from keras.layers import Bidirectional, LSTM, GRU, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "\n",
    "model = get_model(X_meta_feats_df.values, nb_words)\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='Data/model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 30), 10000, (10001, 501))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta_feats_df.shape, src_vocab_size, embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 04:28:46 PM\n",
      "Working on fold - 1\n",
      "Train on 1674 samples, validate on 826 samples\n",
      "Epoch 1/2\n",
      "1674/1674 [==============================] - 81s 49ms/step - loss: 0.2533 - acc: 0.9248 - val_loss: 0.1314 - val_acc: 0.9669\n",
      "2500/2500 [==============================] - 43s 17ms/step\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.655369 \n",
      "\n",
      "*** New High Score (previous: 0.000000) \n",
      "\n",
      "Epoch 2/2\n",
      "1674/1674 [==============================] - 81s 48ms/step - loss: 0.1352 - acc: 0.9614 - val_loss: 0.1216 - val_acc: 0.9673\n",
      "2500/2500 [==============================] - 49s 19ms/step\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.747116 \n",
      "\n",
      "*** New High Score (previous: 0.655369) \n",
      "\n",
      "History is {'val_loss': [0.1314047505001179, 0.12161328042823524], 'val_acc': [0.9669087939054568, 0.9673123436170398], 'loss': [0.2533089115396765, 0.1351990788827874], 'acc': [0.9248307420646276, 0.9613699736418547]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fold - 2\n",
      "Train on 1674 samples, validate on 826 samples\n",
      "Epoch 1/2\n",
      "1674/1674 [==============================] - 110s 66ms/step - loss: 0.2830 - acc: 0.9026 - val_loss: 0.1376 - val_acc: 0.9621\n",
      "2500/2500 [==============================] - 59s 24ms/step\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.486358 \n",
      "\n",
      "*** New High Score (previous: 0.000000) \n",
      "\n",
      "Epoch 2/2\n",
      "1674/1674 [==============================] - 103s 62ms/step - loss: 0.1354 - acc: 0.9605 - val_loss: 0.1013 - val_acc: 0.9683\n",
      "2500/2500 [==============================] - 116s 46ms/step\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.798553 \n",
      "\n",
      "*** New High Score (previous: 0.486358) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "# Checkpoint the weights when validation accuracy improves\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import History \n",
    "\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print('Start time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))\n",
    "\n",
    "batch_size = 32\n",
    "bst_model_path=\"Data\\Weights\\weights.best.hdf5\"\n",
    "\n",
    "# Use epochs=100 with early exiting for best score.\n",
    "epochs = 2\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "# Change to 10\n",
    "num_folds = 2 #number of folds\n",
    "history = History()\n",
    "# predict = np.zeros((test.shape[0],6))\n",
    "\n",
    "# Uncomment for out-of-fold predictions\n",
    "# scores = []\n",
    "# oof_predict = np.zeros((train.shape[0],6))\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n",
    "\n",
    "ctr = 0\n",
    "for train_index, test_index in kf.split(X_train_padded_seq):\n",
    "    ctr += 1\n",
    "    print ('Working on fold - {}'.format(ctr))\n",
    "    \n",
    "    \n",
    "    kfold_y_train, kfold_y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    kfold_X_train = X_train_padded_seq[train_index]\n",
    "    kfold_X_meta_features = X_meta_feats_df.iloc[train_index]\n",
    "    \n",
    "    kfold_X_valid = X_train_padded_seq[test_index]\n",
    "    kfold_X_valid_meta_features = X_meta_feats_df.iloc[test_index] \n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = get_model(X_meta_feats_df.values, nb_words)\n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid, kfold_X_valid_meta_features], kfold_y_test), interval = 1)\n",
    "\n",
    "    # checkpoint\n",
    "#     bst_model_path=\"Data\\Weights\\weights.best.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(bst_model_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    \n",
    "#     callbacks_list = [checkpoint, ra_val]\n",
    "    callbacks_list = [ra_val, history]\n",
    "    \n",
    "#     history = model.fit([kfold_X_train, kfold_X_meta_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "#              callbacks = callbacks_list)\n",
    "    model.fit([kfold_X_train, kfold_X_meta_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "             callbacks = callbacks_list, validation_split=0.33)\n",
    "    gc.collect()\n",
    "    \n",
    "    model.load_weights(bst_model_path)\n",
    "    print('History is {}'.format(history.history))\n",
    "    #     model.load_weights(\"Data\\Weights\\best_weights.h5\")\n",
    "    \n",
    "#     print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plot_history(history)\n",
    "    \n",
    "    \n",
    "#     predict += model.predict([x_test,test_features], batch_size=batch_size,verbose=1) / num_folds\n",
    "    \n",
    "    #gc.collect()\n",
    "    # uncomment for out of fold predictions\n",
    "    #oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n",
    "    #cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n",
    "    \n",
    "    #scores.append(cv_score)\n",
    "    #print('score: ',cv_score)\n",
    "\n",
    "print(\"Done\")\n",
    "#print('Total CV score is {}'.format(np.mean(scores)))  \n",
    "currentDT = datetime.datetime.now()\n",
    "print('End time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-067f13cea0d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "type(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "#Embedding layers takes indices and returns vectors. They are used a lot in NLP tasks. You can think of this embedding layer\n",
    "#as a word2vec. If you give it a number 5 it will return a vector of 5\n",
    "#here we are saying take the top_words that is of 5000 words and create a vector represetation equal embedding_vector_length,\n",
    "#which is of size 32. This means there will be 32 activation units in the embedding layer.\n",
    "#the input is 5000 words and there are 32 activation units, so the # of params are 5000*32=160K\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "#LSTM(100) means we have 100 LSTM cells, each of these 100 LSTM cells takes each of the 32 dim vectors\n",
    "#each of these LSTMs generate an output\n",
    "#the number of params is - there are 32 inputs lets call this 'm' and there are 100 outputs, lets call this 'n'\n",
    "#the formula is 4(nm+n^2+n) - the last n is the biases. LSTM in Keras by default has biases.\n",
    "#4(3200+10000+100)=53200\n",
    "model.add(LSTM(100))\n",
    "#we are taking all the 100 outputs and connecting it to a sigmoid unit to generate y_i_hat.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#for a pictorial representation of this - refer to my notes\n",
    "#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, nb_epoch=10, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_6_5_keras",
   "language": "python",
   "name": "py_3_6_5_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
