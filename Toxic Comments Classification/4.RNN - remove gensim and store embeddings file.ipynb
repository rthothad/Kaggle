{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had some issues to load certain packages in the cloud instance I had, so this notebook will save the embeddings in a separate file, which will be loaded to the cloud instance to train the network\n",
    "\n",
    "# Import and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Tools\\Miniconda3\\envs\\py_3_6_5_keras\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "d:\\Tools\\Miniconda3\\envs\\py_3_6_5_keras\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset contains 159571 rows and 39 columns\n"
     ]
    }
   ],
   "source": [
    "#This dataset contains new features, ascii text converted to unicode and standardized\n",
    "nrows= 5000\n",
    "train = pd.read_csv('Data/train_processed_stage1.csv')\n",
    "print('Train dataset contains {} rows and {} columns'.format(*train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_clean_text_df contains 159571 rows and 2 columns\n",
      "X_meta_feats_df contains 159571 rows and 30 columns\n",
      "y contains 159571 rows and 6 columns\n"
     ]
    }
   ],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "meta_features_cols = ['total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks', 'num_question_marks', \\\n",
    " 'num_punctuation', 'num_symbols', 'num_words', 'num_unique_words', 'words_vs_unique', 'num_smilies', 'ant_slash_n', \\\n",
    " 'nb_fk', 'nb_sk', 'nb_dk', 'nb_you', 'nb_ng', 'nb_mother', 'start_with_columns', 'has_timestamp', 'has_date_long', \\\n",
    " 'has_date_short', 'has_http', 'has_mail', 'has_image', 'has_ip', 'has_emphasize_equal', 'has_emphasize_quotes', \\\n",
    " 'has_star', 'unknown_fasttext']\n",
    "\n",
    "X_clean_text_df = train[['id','clean_text']].fillna('something')\n",
    "X_meta_feats_df = train[meta_features_cols].fillna('something')\n",
    "y = train[class_names]\n",
    "print ('X_clean_text_df contains {} rows and {} columns'.format(*X_clean_text_df.shape))\n",
    "print ('X_meta_feats_df contains {} rows and {} columns'.format(*X_meta_feats_df.shape))\n",
    "print ('y contains {} rows and {} columns'.format(*y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>d aww ! he matches this background colour i am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man i am really not trying to edit war it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>more i can not make any real suggestions on im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                         clean_text\n",
       "0  0000997932d777bf  explanation why the edits made under my userna...\n",
       "1  000103f0d9cfb60f  d aww ! he matches this background colour i am...\n",
       "2  000113f07ec002fd  hey man i am really not trying to edit war it ...\n",
       "3  0001b41b1c6bb37e  more i can not make any real suggestions on im...\n",
       "4  0001d958c54c6e35  you sir are my hero any chance you remember wh..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_length</th>\n",
       "      <th>capitals</th>\n",
       "      <th>caps_vs_length</th>\n",
       "      <th>num_exclamation_marks</th>\n",
       "      <th>num_question_marks</th>\n",
       "      <th>num_punctuation</th>\n",
       "      <th>num_symbols</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>words_vs_unique</th>\n",
       "      <th>...</th>\n",
       "      <th>has_date_long</th>\n",
       "      <th>has_date_short</th>\n",
       "      <th>has_http</th>\n",
       "      <th>has_mail</th>\n",
       "      <th>has_image</th>\n",
       "      <th>has_ip</th>\n",
       "      <th>has_emphasize_equal</th>\n",
       "      <th>has_emphasize_quotes</th>\n",
       "      <th>has_star</th>\n",
       "      <th>unknown_fasttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.220195</td>\n",
       "      <td>-0.001654</td>\n",
       "      <td>0.137572</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>0.346388</td>\n",
       "      <td>-0.154587</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.244618</td>\n",
       "      <td>-0.130379</td>\n",
       "      <td>0.768235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>1.340625</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.477509</td>\n",
       "      <td>-0.094891</td>\n",
       "      <td>0.213346</td>\n",
       "      <td>0.013065</td>\n",
       "      <td>-0.282584</td>\n",
       "      <td>-0.232894</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.506634</td>\n",
       "      <td>-0.571261</td>\n",
       "      <td>1.131109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.272673</td>\n",
       "      <td>-0.136330</td>\n",
       "      <td>-0.371132</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>-0.282584</td>\n",
       "      <td>-0.311200</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.254695</td>\n",
       "      <td>-0.167119</td>\n",
       "      <td>0.573838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.385847</td>\n",
       "      <td>-0.063812</td>\n",
       "      <td>-0.365558</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>-0.282584</td>\n",
       "      <td>-0.154587</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>0.460811</td>\n",
       "      <td>0.622796</td>\n",
       "      <td>-1.009204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.553687</td>\n",
       "      <td>-0.157049</td>\n",
       "      <td>-0.234513</td>\n",
       "      <td>-0.025546</td>\n",
       "      <td>0.346388</td>\n",
       "      <td>-0.389507</td>\n",
       "      <td>-0.107151</td>\n",
       "      <td>-0.546945</td>\n",
       "      <td>-0.644741</td>\n",
       "      <td>1.131109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081772</td>\n",
       "      <td>-0.112575</td>\n",
       "      <td>-0.129103</td>\n",
       "      <td>-0.028225</td>\n",
       "      <td>-0.008183</td>\n",
       "      <td>-0.172487</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.00468</td>\n",
       "      <td>-0.051793</td>\n",
       "      <td>-0.20777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_length  capitals  caps_vs_length  num_exclamation_marks  \\\n",
       "0     -0.220195 -0.001654        0.137572              -0.025546   \n",
       "1     -0.477509 -0.094891        0.213346               0.013065   \n",
       "2     -0.272673 -0.136330       -0.371132              -0.025546   \n",
       "3      0.385847 -0.063812       -0.365558              -0.025546   \n",
       "4     -0.553687 -0.157049       -0.234513              -0.025546   \n",
       "\n",
       "   num_question_marks  num_punctuation  num_symbols  num_words  \\\n",
       "0            0.346388        -0.154587    -0.107151  -0.244618   \n",
       "1           -0.282584        -0.232894    -0.107151  -0.506634   \n",
       "2           -0.282584        -0.311200    -0.107151  -0.254695   \n",
       "3           -0.282584        -0.154587    -0.107151   0.460811   \n",
       "4            0.346388        -0.389507    -0.107151  -0.546945   \n",
       "\n",
       "   num_unique_words  words_vs_unique        ...         has_date_long  \\\n",
       "0         -0.130379         0.768235        ...             -0.081772   \n",
       "1         -0.571261         1.131109        ...             -0.081772   \n",
       "2         -0.167119         0.573838        ...             -0.081772   \n",
       "3          0.622796        -1.009204        ...             -0.081772   \n",
       "4         -0.644741         1.131109        ...             -0.081772   \n",
       "\n",
       "   has_date_short  has_http  has_mail  has_image    has_ip  \\\n",
       "0       -0.112575 -0.129103 -0.028225  -0.008183  1.340625   \n",
       "1       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "2       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "3       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "4       -0.112575 -0.129103 -0.028225  -0.008183 -0.172487   \n",
       "\n",
       "   has_emphasize_equal  has_emphasize_quotes  has_star  unknown_fasttext  \n",
       "0            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "1            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "2            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "3            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "4            -0.044091              -0.00468 -0.051793          -0.20777  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta_feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0\n",
       "5      0             0        0       0       0              0\n",
       "6      1             1        1       0       1              0\n",
       "7      0             0        0       0       0              0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert clean_text to vectors/tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This allows to vectorize a text corpus, by turning each text into a sequence of integers \n",
    "# src_vocab_size = 10000\n",
    "src_vocab_size = 283759\n",
    "def create_tokenizer():\n",
    "    #consider only the top 10000 words in the dataset\n",
    "    tokenizer = text.Tokenizer(num_words=src_vocab_size)\n",
    "    tokenizer.fit_on_texts(list(X_clean_text_df['clean_text']))\n",
    "    return tokenizer\n",
    "\n",
    "def find_max_len(percentile=0.9):\n",
    "    #since each of the reviews can be of varying length. we will convert them to be of same length by padding.\n",
    "    #lets find out what should be the length of each reviews\n",
    "    #find the length\n",
    "    X_clean_text_df['text_length'] = X_clean_text_df['clean_text'].apply(len)\n",
    "\n",
    "    # use the 90th percentile to pad sequences\n",
    "    maxlen = int(X_clean_text_df.text_length.quantile(percentile)) # 90th percentile\n",
    "    return maxlen\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length):\n",
    "    #map words to integers as needed for modelling\n",
    "    X_train_sequence = tokenizer.texts_to_sequences(X_clean_text_df['clean_text'])\n",
    "\n",
    "    # pad sequences with 0 values\n",
    "    #it will take each review and make them to be of 836 words.\n",
    "    #if the review is less than 836 words it will pad zeroes at the start and append the reviews to the end.\n",
    "    #if the review is more than 836 words, the words beyond 600 will be truncated\n",
    "    X_train_padded_seq = sequence.pad_sequences(X_train_sequence, maxlen=length)\n",
    "    return X_train_padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens created from the training dataset is 169494\n",
      "Sequences will be of length - 913\n"
     ]
    }
   ],
   "source": [
    "tokenizer = create_tokenizer()\n",
    "print('Number of unique tokens created from the training dataset is {}'.format(len(tokenizer.word_index)))\n",
    "\n",
    "maxlen = find_max_len(0.91)\n",
    "print('Sequences will be of length - {}'.format(maxlen))\n",
    "\n",
    "X_train_padded_seq = encode_sequences(tokenizer, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   41, 3220,   88],\n",
       "       [   0,    0,    0, ...,   37,  934,  179]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded_seq[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Build an embedding matrix that can be loaded into an Embedding layer. It must be a matrix of shape (src_vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and -0.1081 0.0191 0.0354 0.0127 0.0664 -0.0126 -0.1882 0.0631 -0.2306 0.0095 0.0917 0.1513 0.0558 -0.0643 -0.0288 -0.0447 0.1603 0.0613 0.0349 -0.0578 0.0003 -0.1399 0.0163 -0.0419 -0.0487 0.0057 0.0017 -0.0268 -0.0170 0.1045 0.0815 -0.0060 0.0635 -0.1710 0.0276 -0.0230 0.0824 0.0304 -0.1595 0.0851 -0.0556 0.0329 -0.0921 0.0196 -0.0404 0.0361 -0.0843 0.1014 -0.0393 -0.1805 -0.0073 0.2247 -0.0621 -0.0575 -0.0316 0.0198 0.0602 0.1312 -0.1278 0.0177 -0.0600 -0.2904 -0.0465 -0.1078 -0.0701 -0.0497 -0.1102 0.0156 -0.0595 0.0762 0.2638 0.0353 0.0745 0.0361 -0.0561 -0.1179 -0.4751 0.0458 -0.0672 -0.0022 0.9264 0.0101 -0.1085 0.0669 -0.0417 -0.0428 0.0391 -0.0581 0.0107 -0.0873 -0.0016 0.5711 0.0345 0.0930 -0.0647 -0.0820 0.0587 -0.1307 0.0480 0.0226 0.0115 0.0296 -0.1675 0.0399 0.2511 0.0423 0.1299 -0.0336 -0.0977 0.0146 0.3527 0.0122 0.1031 -0.0657 -0.3704 0.0445 0.1242 -0.0031 0.0255 -0.0109 0.1575 0.0288 0.2549 -0.0280 0.0462 -0.0282 -0.0154 0.0405 0.3191 0.0113 -0.0136 0.0182 -0.0385 -0.0499 -0.0810 -0.0134 -0.1413 0.0566 -0.0423 -0.0609 0.1063 0.0386 -0.0375 0.2714 -0.0665 -0.0219 0.0579 0.0550 0.0772 0.2024 -0.0365 -0.0764 0.0318 0.0004 -0.0424 -0.0579 0.0428 0.0211 -0.0061 0.0243 0.5702 0.0798 -0.0502 -0.0195 0.0242 -0.0339 -0.0441 -0.1095 0.0426 -0.0234 -0.0057 -0.0209 -0.0903 -0.0654 0.1559 0.0051 0.0037 -0.0008 -0.0050 0.0508 0.0105 0.0251 0.0270 0.0039 0.0621 0.0519 -0.0156 -0.1285 -0.0564 -0.0263 0.0401 0.0372 -0.1040 -0.0674 0.0866 -0.0033 -0.0076 0.0064 -0.0114 -0.0239 0.0135 0.0801 0.0171 0.0425 -0.0499 0.0615 0.0355 -0.0833 0.0378 0.0288 -0.0392 0.5679 -0.0120 0.0699 -0.0893 -0.0105 0.0219 -0.0136 0.1063 -0.0014 -0.0062 -0.0617 0.0357 0.0030 -0.0414 -0.0634 0.0701 0.0501 0.0241 0.0379 -0.0234 -0.0116 0.0391 -0.0169 -0.0019 0.0731 0.0597 0.0191 -0.0262 -0.0256 0.0631 -0.0261 0.0185 0.0100 0.1126 -0.0487 0.1545 0.0438 0.0147 -0.0278 -0.0493 -0.0866 -0.0281 -0.0955 0.0879 0.0194 -0.0323 -0.0126 -0.0810 0.0088 0.0002 0.0059 0.0858 0.0024 0.0401 -0.0205 0.0304 0.0678 0.0566 0.0097 0.0039 -0.0968 0.0079 -0.3838 -0.0642 -0.0417 -0.0666 0.0337 0.0053 -0.0263 -0.0310 0.2002 -0.0096 0.0796 0.0743 0.0709 -0.0077 -0.0169 -0.0083 -0.0322 0.0608 0.0227 -0.0306 0.3945 0.0200 -0.0613 -0.0611 0.1104 0.0475 -0.0599 \n",
      "\n",
      "('and', array([-1.081e-01,  1.910e-02,  3.540e-02,  1.270e-02,  6.640e-02,\n",
      "       -1.260e-02, -1.882e-01,  6.310e-02, -2.306e-01,  9.500e-03,\n",
      "        9.170e-02,  1.513e-01,  5.580e-02, -6.430e-02, -2.880e-02,\n",
      "       -4.470e-02,  1.603e-01,  6.130e-02,  3.490e-02, -5.780e-02,\n",
      "        3.000e-04, -1.399e-01,  1.630e-02, -4.190e-02, -4.870e-02,\n",
      "        5.700e-03,  1.700e-03, -2.680e-02, -1.700e-02,  1.045e-01,\n",
      "        8.150e-02, -6.000e-03,  6.350e-02, -1.710e-01,  2.760e-02,\n",
      "       -2.300e-02,  8.240e-02,  3.040e-02, -1.595e-01,  8.510e-02,\n",
      "       -5.560e-02,  3.290e-02, -9.210e-02,  1.960e-02, -4.040e-02,\n",
      "        3.610e-02, -8.430e-02,  1.014e-01, -3.930e-02, -1.805e-01,\n",
      "       -7.300e-03,  2.247e-01, -6.210e-02, -5.750e-02, -3.160e-02,\n",
      "        1.980e-02,  6.020e-02,  1.312e-01, -1.278e-01,  1.770e-02,\n",
      "       -6.000e-02, -2.904e-01, -4.650e-02, -1.078e-01, -7.010e-02,\n",
      "       -4.970e-02, -1.102e-01,  1.560e-02, -5.950e-02,  7.620e-02,\n",
      "        2.638e-01,  3.530e-02,  7.450e-02,  3.610e-02, -5.610e-02,\n",
      "       -1.179e-01, -4.751e-01,  4.580e-02, -6.720e-02, -2.200e-03,\n",
      "        9.264e-01,  1.010e-02, -1.085e-01,  6.690e-02, -4.170e-02,\n",
      "       -4.280e-02,  3.910e-02, -5.810e-02,  1.070e-02, -8.730e-02,\n",
      "       -1.600e-03,  5.711e-01,  3.450e-02,  9.300e-02, -6.470e-02,\n",
      "       -8.200e-02,  5.870e-02, -1.307e-01,  4.800e-02,  2.260e-02,\n",
      "        1.150e-02,  2.960e-02, -1.675e-01,  3.990e-02,  2.511e-01,\n",
      "        4.230e-02,  1.299e-01, -3.360e-02, -9.770e-02,  1.460e-02,\n",
      "        3.527e-01,  1.220e-02,  1.031e-01, -6.570e-02, -3.704e-01,\n",
      "        4.450e-02,  1.242e-01, -3.100e-03,  2.550e-02, -1.090e-02,\n",
      "        1.575e-01,  2.880e-02,  2.549e-01, -2.800e-02,  4.620e-02,\n",
      "       -2.820e-02, -1.540e-02,  4.050e-02,  3.191e-01,  1.130e-02,\n",
      "       -1.360e-02,  1.820e-02, -3.850e-02, -4.990e-02, -8.100e-02,\n",
      "       -1.340e-02, -1.413e-01,  5.660e-02, -4.230e-02, -6.090e-02,\n",
      "        1.063e-01,  3.860e-02, -3.750e-02,  2.714e-01, -6.650e-02,\n",
      "       -2.190e-02,  5.790e-02,  5.500e-02,  7.720e-02,  2.024e-01,\n",
      "       -3.650e-02, -7.640e-02,  3.180e-02,  4.000e-04, -4.240e-02,\n",
      "       -5.790e-02,  4.280e-02,  2.110e-02, -6.100e-03,  2.430e-02,\n",
      "        5.702e-01,  7.980e-02, -5.020e-02, -1.950e-02,  2.420e-02,\n",
      "       -3.390e-02, -4.410e-02, -1.095e-01,  4.260e-02, -2.340e-02,\n",
      "       -5.700e-03, -2.090e-02, -9.030e-02, -6.540e-02,  1.559e-01,\n",
      "        5.100e-03,  3.700e-03, -8.000e-04, -5.000e-03,  5.080e-02,\n",
      "        1.050e-02,  2.510e-02,  2.700e-02,  3.900e-03,  6.210e-02,\n",
      "        5.190e-02, -1.560e-02, -1.285e-01, -5.640e-02, -2.630e-02,\n",
      "        4.010e-02,  3.720e-02, -1.040e-01, -6.740e-02,  8.660e-02,\n",
      "       -3.300e-03, -7.600e-03,  6.400e-03, -1.140e-02, -2.390e-02,\n",
      "        1.350e-02,  8.010e-02,  1.710e-02,  4.250e-02, -4.990e-02,\n",
      "        6.150e-02,  3.550e-02, -8.330e-02,  3.780e-02,  2.880e-02,\n",
      "       -3.920e-02,  5.679e-01, -1.200e-02,  6.990e-02, -8.930e-02,\n",
      "       -1.050e-02,  2.190e-02, -1.360e-02,  1.063e-01, -1.400e-03,\n",
      "       -6.200e-03, -6.170e-02,  3.570e-02,  3.000e-03, -4.140e-02,\n",
      "       -6.340e-02,  7.010e-02,  5.010e-02,  2.410e-02,  3.790e-02,\n",
      "       -2.340e-02, -1.160e-02,  3.910e-02, -1.690e-02, -1.900e-03,\n",
      "        7.310e-02,  5.970e-02,  1.910e-02, -2.620e-02, -2.560e-02,\n",
      "        6.310e-02, -2.610e-02,  1.850e-02,  1.000e-02,  1.126e-01,\n",
      "       -4.870e-02,  1.545e-01,  4.380e-02,  1.470e-02, -2.780e-02,\n",
      "       -4.930e-02, -8.660e-02, -2.810e-02, -9.550e-02,  8.790e-02,\n",
      "        1.940e-02, -3.230e-02, -1.260e-02, -8.100e-02,  8.800e-03,\n",
      "        2.000e-04,  5.900e-03,  8.580e-02,  2.400e-03,  4.010e-02,\n",
      "       -2.050e-02,  3.040e-02,  6.780e-02,  5.660e-02,  9.700e-03,\n",
      "        3.900e-03, -9.680e-02,  7.900e-03, -3.838e-01, -6.420e-02,\n",
      "       -4.170e-02, -6.660e-02,  3.370e-02,  5.300e-03, -2.630e-02,\n",
      "       -3.100e-02,  2.002e-01, -9.600e-03,  7.960e-02,  7.430e-02,\n",
      "        7.090e-02, -7.700e-03, -1.690e-02, -8.300e-03, -3.220e-02,\n",
      "        6.080e-02,  2.270e-02, -3.060e-02,  3.945e-01,  2.000e-02,\n",
      "       -6.130e-02, -6.110e-02,  1.104e-01,  4.750e-02, -5.990e-02],\n",
      "      dtype=float32))\n",
      "to -0.0175 -0.2189 0.0353 0.0345 0.0913 0.0269 -0.1670 -0.2759 0.4283 0.0349 0.0132 0.1078 -0.0558 -0.0215 -0.0389 0.0379 -0.1180 0.0164 0.0631 0.0459 0.0331 -0.0176 0.0010 0.0176 0.0351 -0.0545 0.0430 -0.0300 0.0358 0.2588 0.0576 -0.0854 0.0632 0.0255 0.0008 0.0152 -0.0843 -0.0249 0.0651 -0.0961 -0.0107 0.1649 0.0003 -0.2875 -0.0388 -0.0212 -0.0973 0.0301 -0.0474 0.9173 -0.0024 0.4422 0.1367 -0.0177 -0.0591 -0.0069 0.0692 -0.3956 -0.0486 0.0251 0.1376 0.1816 -0.0164 -0.2103 0.0156 -0.0449 -0.1004 0.0399 -0.0847 -0.0565 -0.2697 0.0051 -0.0441 0.1862 -0.0143 -0.1114 -0.7036 0.0162 0.1955 -0.0225 0.6761 0.0295 -0.0039 -0.0450 -0.0383 0.0258 0.0464 -0.0243 0.0243 0.0037 0.0031 0.6531 0.0330 0.0092 0.0026 0.0351 -0.3749 -0.0402 -0.2596 0.0326 0.0055 -0.0039 0.1751 0.0420 0.1438 -0.0109 0.0236 -0.0084 -0.0188 0.1287 0.0624 0.1152 0.0244 0.0207 -0.3837 0.2028 0.1272 0.0179 0.1020 0.0433 -0.0650 0.0281 0.1151 -0.0074 0.0174 -0.0252 0.0533 0.0659 0.2928 -0.0211 -0.0252 -0.0004 -0.0462 -0.0615 0.3430 -0.0705 -0.0938 0.0509 0.0076 -0.2799 0.1191 0.0606 -0.0089 0.1064 -0.0829 -0.0044 -0.1018 -0.0100 0.0765 0.1046 0.0296 -0.0144 -0.0241 0.0203 -0.0177 -0.0347 0.0244 0.0044 0.0060 0.0126 0.4667 0.1586 -0.0376 -0.0566 -0.0387 0.0108 -0.0083 -0.0498 0.0121 -0.0458 0.0430 0.0027 -0.0922 0.0159 0.0770 -0.0638 0.0435 -0.0152 0.0018 -0.0024 -0.2509 -0.0197 0.0097 0.0214 0.0603 -0.0105 -0.0940 -0.4068 -0.2884 0.0445 0.0020 0.0126 -0.0744 -0.0678 0.0483 -0.0919 0.0431 -0.0401 0.0313 -0.0202 0.0297 0.0010 0.0224 0.0575 0.0349 -0.0103 0.0015 0.0122 0.0415 -0.2559 0.0132 0.5005 -0.0074 0.0416 -0.0839 0.0086 -0.0155 -0.0089 -0.0076 0.0295 -0.0277 -0.0942 0.0236 0.0153 0.0798 -0.0475 -0.0431 0.0369 0.0396 0.0146 0.0790 0.0716 0.0334 -0.0462 0.0121 0.0620 0.0960 -0.0337 -0.0617 0.0416 -0.0236 0.0054 -0.1031 -0.0331 -0.3805 -0.0750 0.4056 0.0209 0.0106 -0.0344 0.0162 -0.1616 0.0356 -0.0108 0.1220 0.0484 0.0353 0.0462 -0.0053 0.0145 0.0615 0.0071 -0.0241 -0.0327 0.0071 -0.1290 0.0202 0.0345 0.0275 -0.0193 -0.0033 -0.4104 0.0505 -0.1699 -0.0398 -0.0099 -0.3296 -0.0402 0.1344 0.0010 0.0336 0.1412 0.0023 -0.0556 0.0348 -0.0625 -0.0228 -0.0015 0.0732 0.0362 0.0642 0.0136 0.0367 0.1735 0.0217 0.0591 0.0166 -0.2846 0.0509 0.0229 \n",
      "\n",
      "('to', array([-1.750e-02, -2.189e-01,  3.530e-02,  3.450e-02,  9.130e-02,\n",
      "        2.690e-02, -1.670e-01, -2.759e-01,  4.283e-01,  3.490e-02,\n",
      "        1.320e-02,  1.078e-01, -5.580e-02, -2.150e-02, -3.890e-02,\n",
      "        3.790e-02, -1.180e-01,  1.640e-02,  6.310e-02,  4.590e-02,\n",
      "        3.310e-02, -1.760e-02,  1.000e-03,  1.760e-02,  3.510e-02,\n",
      "       -5.450e-02,  4.300e-02, -3.000e-02,  3.580e-02,  2.588e-01,\n",
      "        5.760e-02, -8.540e-02,  6.320e-02,  2.550e-02,  8.000e-04,\n",
      "        1.520e-02, -8.430e-02, -2.490e-02,  6.510e-02, -9.610e-02,\n",
      "       -1.070e-02,  1.649e-01,  3.000e-04, -2.875e-01, -3.880e-02,\n",
      "       -2.120e-02, -9.730e-02,  3.010e-02, -4.740e-02,  9.173e-01,\n",
      "       -2.400e-03,  4.422e-01,  1.367e-01, -1.770e-02, -5.910e-02,\n",
      "       -6.900e-03,  6.920e-02, -3.956e-01, -4.860e-02,  2.510e-02,\n",
      "        1.376e-01,  1.816e-01, -1.640e-02, -2.103e-01,  1.560e-02,\n",
      "       -4.490e-02, -1.004e-01,  3.990e-02, -8.470e-02, -5.650e-02,\n",
      "       -2.697e-01,  5.100e-03, -4.410e-02,  1.862e-01, -1.430e-02,\n",
      "       -1.114e-01, -7.036e-01,  1.620e-02,  1.955e-01, -2.250e-02,\n",
      "        6.761e-01,  2.950e-02, -3.900e-03, -4.500e-02, -3.830e-02,\n",
      "        2.580e-02,  4.640e-02, -2.430e-02,  2.430e-02,  3.700e-03,\n",
      "        3.100e-03,  6.531e-01,  3.300e-02,  9.200e-03,  2.600e-03,\n",
      "        3.510e-02, -3.749e-01, -4.020e-02, -2.596e-01,  3.260e-02,\n",
      "        5.500e-03, -3.900e-03,  1.751e-01,  4.200e-02,  1.438e-01,\n",
      "       -1.090e-02,  2.360e-02, -8.400e-03, -1.880e-02,  1.287e-01,\n",
      "        6.240e-02,  1.152e-01,  2.440e-02,  2.070e-02, -3.837e-01,\n",
      "        2.028e-01,  1.272e-01,  1.790e-02,  1.020e-01,  4.330e-02,\n",
      "       -6.500e-02,  2.810e-02,  1.151e-01, -7.400e-03,  1.740e-02,\n",
      "       -2.520e-02,  5.330e-02,  6.590e-02,  2.928e-01, -2.110e-02,\n",
      "       -2.520e-02, -4.000e-04, -4.620e-02, -6.150e-02,  3.430e-01,\n",
      "       -7.050e-02, -9.380e-02,  5.090e-02,  7.600e-03, -2.799e-01,\n",
      "        1.191e-01,  6.060e-02, -8.900e-03,  1.064e-01, -8.290e-02,\n",
      "       -4.400e-03, -1.018e-01, -1.000e-02,  7.650e-02,  1.046e-01,\n",
      "        2.960e-02, -1.440e-02, -2.410e-02,  2.030e-02, -1.770e-02,\n",
      "       -3.470e-02,  2.440e-02,  4.400e-03,  6.000e-03,  1.260e-02,\n",
      "        4.667e-01,  1.586e-01, -3.760e-02, -5.660e-02, -3.870e-02,\n",
      "        1.080e-02, -8.300e-03, -4.980e-02,  1.210e-02, -4.580e-02,\n",
      "        4.300e-02,  2.700e-03, -9.220e-02,  1.590e-02,  7.700e-02,\n",
      "       -6.380e-02,  4.350e-02, -1.520e-02,  1.800e-03, -2.400e-03,\n",
      "       -2.509e-01, -1.970e-02,  9.700e-03,  2.140e-02,  6.030e-02,\n",
      "       -1.050e-02, -9.400e-02, -4.068e-01, -2.884e-01,  4.450e-02,\n",
      "        2.000e-03,  1.260e-02, -7.440e-02, -6.780e-02,  4.830e-02,\n",
      "       -9.190e-02,  4.310e-02, -4.010e-02,  3.130e-02, -2.020e-02,\n",
      "        2.970e-02,  1.000e-03,  2.240e-02,  5.750e-02,  3.490e-02,\n",
      "       -1.030e-02,  1.500e-03,  1.220e-02,  4.150e-02, -2.559e-01,\n",
      "        1.320e-02,  5.005e-01, -7.400e-03,  4.160e-02, -8.390e-02,\n",
      "        8.600e-03, -1.550e-02, -8.900e-03, -7.600e-03,  2.950e-02,\n",
      "       -2.770e-02, -9.420e-02,  2.360e-02,  1.530e-02,  7.980e-02,\n",
      "       -4.750e-02, -4.310e-02,  3.690e-02,  3.960e-02,  1.460e-02,\n",
      "        7.900e-02,  7.160e-02,  3.340e-02, -4.620e-02,  1.210e-02,\n",
      "        6.200e-02,  9.600e-02, -3.370e-02, -6.170e-02,  4.160e-02,\n",
      "       -2.360e-02,  5.400e-03, -1.031e-01, -3.310e-02, -3.805e-01,\n",
      "       -7.500e-02,  4.056e-01,  2.090e-02,  1.060e-02, -3.440e-02,\n",
      "        1.620e-02, -1.616e-01,  3.560e-02, -1.080e-02,  1.220e-01,\n",
      "        4.840e-02,  3.530e-02,  4.620e-02, -5.300e-03,  1.450e-02,\n",
      "        6.150e-02,  7.100e-03, -2.410e-02, -3.270e-02,  7.100e-03,\n",
      "       -1.290e-01,  2.020e-02,  3.450e-02,  2.750e-02, -1.930e-02,\n",
      "       -3.300e-03, -4.104e-01,  5.050e-02, -1.699e-01, -3.980e-02,\n",
      "       -9.900e-03, -3.296e-01, -4.020e-02,  1.344e-01,  1.000e-03,\n",
      "        3.360e-02,  1.412e-01,  2.300e-03, -5.560e-02,  3.480e-02,\n",
      "       -6.250e-02, -2.280e-02, -1.500e-03,  7.320e-02,  3.620e-02,\n",
      "        6.420e-02,  1.360e-02,  3.670e-02,  1.735e-01,  2.170e-02,\n",
      "        5.910e-02,  1.660e-02, -2.846e-01,  5.090e-02,  2.290e-02],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "#understand what the file contains\n",
    "EMBEDDING_FILE_FASTTEXT=\"Data/embeddings/crawl-300d-2M.vec\"\n",
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "cnt = 1\n",
    "for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'):\n",
    "    if cnt > 4:\n",
    "        print(o)\n",
    "        print(get_coefs(*o.rstrip().rsplit(' ')))\n",
    "    cnt += 1\n",
    "    if cnt > 6: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def create_embeddings(source_file):\n",
    "    embeddings = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(source_file,encoding='utf-8'))\n",
    "    return embeddings\n",
    "\n",
    "def save_embeddings_to_file(source_file, target_file_name):\n",
    "    with open(target_file_name, 'wb') as handle:\n",
    "        pickle.dump(source_file, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_from_pickle_file():\n",
    "    if os.path.isfile(EMBEDDING_DICT_FASTTEXT):\n",
    "        #load file\n",
    "        with open(EMBEDDING_DICT_FASTTEXT, 'rb') as handle:\n",
    "            embeddings_index_ft = pickle.load(handle)\n",
    "    else:\n",
    "        embeddings_index_ft = create_embeddings(EMBEDDING_FILE_FASTTEXT)\n",
    "        save_embeddings_to_file(embeddings_index_ft, EMBEDDING_DICT_FASTTEXT)\n",
    "        \n",
    "    if os.path.isfile(EMBEDDING_DICT_TWITTER):\n",
    "        #load file\n",
    "        with open(EMBEDDING_DICT_TWITTER, 'rb') as handle:\n",
    "            embeddings_index_tw = pickle.load(handle)\n",
    "    else:\n",
    "        embeddings_index_tw = create_embeddings(EMBEDDING_FILE_TWITTER)\n",
    "        save_embeddings_to_file(embeddings_index_tw, EMBEDDING_DICT_TWITTER)\n",
    "    return embeddings_index_ft, embeddings_index_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 word vectors from fasttext embeddings.\n",
      "Loaded 1193514 word vectors from twitter embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Load the FastText Web Crawl vectors\n",
    "# The emebedding file contains words and its corresponding vectors. \n",
    "# For instance the word and will be stored in embedding file as\n",
    "# (and -0.1081 0.0191 0.0354 0.0127 0.0664 -0.0126 -0.1882 0.0631 -0.2306 0.0095 0.0917 0.1513)\n",
    "# we will create a dictionary where the word will be the key and the vectors will be an array\n",
    "EMBEDDING_FILE_FASTTEXT=\"Data/embeddings/crawl-300d-2M.vec\"\n",
    "EMBEDDING_FILE_TWITTER=\"Data/embeddings/glove.twitter.27B.200d.txt\"\n",
    "\n",
    "EMBEDDING_DICT_FASTTEXT=\"Data/embeddings/embeddings_index_ft.pickle\"\n",
    "EMBEDDING_DICT_TWITTER=\"Data/embeddings/embeddings_index_tw.pickle\"\n",
    "\n",
    "# load the fasttext and twitter word embeddings file into memory as a dictionary of word to embedding array.\n",
    "#This process is slow so it is better to save the files\n",
    "embeddings_index_ft, embeddings_index_tw = load_embeddings_from_pickle_file()\n",
    "print('Loaded {} word vectors from fasttext embeddings.'.format(len(embeddings_index_ft)))\n",
    "print('Loaded {} word vectors from twitter embeddings.'.format(len(embeddings_index_tw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 11:41:57 AM\n",
      "End time is 11:51:52 AM\n"
     ]
    }
   ],
   "source": [
    "#load spelling model. This will used to correct words that are not in the fasttext embeddings. \n",
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print('Start time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))\n",
    "\n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print('End time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spell_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is  based on: Spellchecker using Word2vec by CPMP\n",
    "# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "\n",
    "#get the list of words\n",
    "words = spell_model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "#word and an index as its value\n",
    "WORDS = w_rank\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "word_index = tokenizer.word_index\n",
    "i = 1\n",
    "for k,v in word_index.items():\n",
    "    print(k,v)\n",
    "    i +=1\n",
    "    if i > 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169494, 283759)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index), src_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 12:04:31 PM\n",
      "150000 more to go\n",
      "125000 more to go\n",
      "100000 more to go\n",
      "75000 more to go\n",
      "50000 more to go\n",
      "25000 more to go\n",
      "End time is 02:21:47 PM\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "currentDT = datetime.datetime.now()\n",
    "print('Start time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))\n",
    "\n",
    "\n",
    "#this contain the word and its index, from the training dataset such as\n",
    "# the 1\n",
    "# to 2\n",
    "# i 3\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#len(word_index) is 22966 and src_vocab_size is 10000\n",
    "nb_words = min(src_vocab_size, len(word_index)) + 1\n",
    "\n",
    "\n",
    "# we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating \n",
    "# all unique words in the word_index and locating the embedding weight vector from the loaded fasttext embedding. \n",
    "# The result is a matrix of weights only for words we will see during training.\n",
    "# if the word is not found in the fasttext embedding we will try to correct the spelling and check\n",
    "embedding_matrix = np.zeros((nb_words, 501))\n",
    "\n",
    "#get embeddings for the word 'something' from twitter \n",
    "something_tw = embeddings_index_tw.get(\"something\")\n",
    "\n",
    "#get embeddings for the word 'something' from fasttext \n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((501,))\n",
    "something[:300,] = something_ft\n",
    "something[300:500,] = something_tw\n",
    "something[500,] = 0\n",
    "\n",
    "def all_caps(word):\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix, i, word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None: \n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_matrix[i,:300] = embedding_vector_ft\n",
    "        embedding_matrix[i,500] = last_value\n",
    "        embedding_vector_tw = embeddings_index_tw.get(word)\n",
    "        if embedding_vector_tw is not None:\n",
    "            embedding_matrix[i,300:500] = embedding_vector_tw\n",
    "\n",
    "            \n",
    "# Fasttext vector is used by itself if there is no glove vector but not the other way around.\n",
    "ctr = len(word_index)\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i >= src_vocab_size: continue\n",
    "    \n",
    "    if (ctr % 25000==0): print('{} more to go'.format(ctr))\n",
    "    ctr -= 1\n",
    "    \n",
    "    #is the word available in fasttext, create embeddings from fasttext vectors\n",
    "    #if not try to correct the word\n",
    "    if embeddings_index_ft.get(word) is not None:\n",
    "        embed_word(embedding_matrix,i,word)\n",
    "    else:\n",
    "        # change to > 20 for better score.\n",
    "        if len(word) > 20:\n",
    "            #use the something vectors\n",
    "            embedding_matrix[i] = something\n",
    "        else:\n",
    "            word2 = correction(word)\n",
    "            if embeddings_index_ft.get(word2) is not None:\n",
    "                embed_word(embedding_matrix,i,word2)\n",
    "            else:\n",
    "                word2 = correction(singlify(word))\n",
    "                if embeddings_index_ft.get(word2) is not None:\n",
    "                    embed_word(embedding_matrix,i,word2)\n",
    "                else:\n",
    "                    embedding_matrix[i] = something   \n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print('End time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time is 02:42:56 PM\n",
      "End time is 02:43:03 PM\n"
     ]
    }
   ],
   "source": [
    "currentDT = datetime.datetime.now()\n",
    "print('Start time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))\n",
    "\n",
    "target_file_name = \"Data/embeddings/embeddings_matrix.pickle\"\n",
    "source_file = embedding_matrix\n",
    "\n",
    "with open(target_file_name, 'wb') as handle:\n",
    "    pickle.dump(source_file, handle)\n",
    "\n",
    "currentDT = datetime.datetime.now()\n",
    "print('End time is {}'.format(currentDT.strftime(\"%I:%M:%S %p\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_6_5_keras",
   "language": "python",
   "name": "py_3_6_5_keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
